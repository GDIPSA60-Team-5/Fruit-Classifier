{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520 images belonging to 4 classes.\n",
      "Found 90 images belonging to 4 classes.\n",
      "Found 60 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\myass\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 5s/step - accuracy: 0.1250 - loss: 1.4092"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\myass\\lib\\site-packages\\PIL\\Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769ms/step - accuracy: 0.3017 - loss: 1.3522\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53333, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.3041 - loss: 1.3507 - val_accuracy: 0.5333 - val_loss: 1.2095\n",
      "Epoch 2/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661ms/step - accuracy: 0.4164 - loss: 1.2476\n",
      "Epoch 2: val_accuracy improved from 0.53333 to 0.58889, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 794ms/step - accuracy: 0.4195 - loss: 1.2455 - val_accuracy: 0.5889 - val_loss: 1.1358\n",
      "Epoch 3/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544ms/step - accuracy: 0.5035 - loss: 1.1768\n",
      "Epoch 3: val_accuracy improved from 0.58889 to 0.71111, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 673ms/step - accuracy: 0.5056 - loss: 1.1746 - val_accuracy: 0.7111 - val_loss: 1.0522\n",
      "Epoch 4/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526ms/step - accuracy: 0.6679 - loss: 1.0435\n",
      "Epoch 4: val_accuracy improved from 0.71111 to 0.76667, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 658ms/step - accuracy: 0.6675 - loss: 1.0440 - val_accuracy: 0.7667 - val_loss: 0.9960\n",
      "Epoch 5/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524ms/step - accuracy: 0.6903 - loss: 0.9925\n",
      "Epoch 5: val_accuracy improved from 0.76667 to 0.78889, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 658ms/step - accuracy: 0.6903 - loss: 0.9925 - val_accuracy: 0.7889 - val_loss: 0.9276\n",
      "Epoch 6/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - accuracy: 0.7755 - loss: 0.8966\n",
      "Epoch 6: val_accuracy improved from 0.78889 to 0.83333, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 675ms/step - accuracy: 0.7745 - loss: 0.8971 - val_accuracy: 0.8333 - val_loss: 0.8655\n",
      "Epoch 7/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559ms/step - accuracy: 0.7782 - loss: 0.8873\n",
      "Epoch 7: val_accuracy did not improve from 0.83333\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 677ms/step - accuracy: 0.7781 - loss: 0.8868 - val_accuracy: 0.8222 - val_loss: 0.8306\n",
      "Epoch 8/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - accuracy: 0.7961 - loss: 0.8518\n",
      "Epoch 8: val_accuracy improved from 0.83333 to 0.86667, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 671ms/step - accuracy: 0.7971 - loss: 0.8502 - val_accuracy: 0.8667 - val_loss: 0.7880\n",
      "Epoch 9/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - accuracy: 0.8568 - loss: 0.7595\n",
      "Epoch 9: val_accuracy improved from 0.86667 to 0.87778, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 661ms/step - accuracy: 0.8558 - loss: 0.7599 - val_accuracy: 0.8778 - val_loss: 0.7407\n",
      "Epoch 10/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518ms/step - accuracy: 0.9007 - loss: 0.7259\n",
      "Epoch 10: val_accuracy improved from 0.87778 to 0.92222, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 650ms/step - accuracy: 0.8996 - loss: 0.7258 - val_accuracy: 0.9222 - val_loss: 0.7009\n",
      "Epoch 11/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530ms/step - accuracy: 0.9097 - loss: 0.6601\n",
      "Epoch 11: val_accuracy did not improve from 0.92222\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 645ms/step - accuracy: 0.9081 - loss: 0.6613 - val_accuracy: 0.9111 - val_loss: 0.6485\n",
      "Epoch 12/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555ms/step - accuracy: 0.8876 - loss: 0.6730\n",
      "Epoch 12: val_accuracy did not improve from 0.92222\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 673ms/step - accuracy: 0.8874 - loss: 0.6720 - val_accuracy: 0.9000 - val_loss: 0.6480\n",
      "Epoch 13/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - accuracy: 0.8767 - loss: 0.6652\n",
      "Epoch 13: val_accuracy did not improve from 0.92222\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 645ms/step - accuracy: 0.8777 - loss: 0.6635 - val_accuracy: 0.8778 - val_loss: 0.6128\n",
      "Epoch 14/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520ms/step - accuracy: 0.9236 - loss: 0.5897\n",
      "Epoch 14: val_accuracy did not improve from 0.92222\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 634ms/step - accuracy: 0.9232 - loss: 0.5905 - val_accuracy: 0.9111 - val_loss: 0.5704\n",
      "Epoch 15/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534ms/step - accuracy: 0.9240 - loss: 0.5736\n",
      "Epoch 15: val_accuracy did not improve from 0.92222\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 650ms/step - accuracy: 0.9237 - loss: 0.5735 - val_accuracy: 0.8778 - val_loss: 0.5880\n",
      "Epoch 16/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557ms/step - accuracy: 0.8961 - loss: 0.5651\n",
      "Epoch 16: val_accuracy improved from 0.92222 to 0.94444, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 690ms/step - accuracy: 0.8964 - loss: 0.5645 - val_accuracy: 0.9444 - val_loss: 0.5282\n",
      "Epoch 17/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527ms/step - accuracy: 0.9181 - loss: 0.5178\n",
      "Epoch 17: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 642ms/step - accuracy: 0.9185 - loss: 0.5179 - val_accuracy: 0.9222 - val_loss: 0.5211\n",
      "Epoch 18/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532ms/step - accuracy: 0.9241 - loss: 0.5073\n",
      "Epoch 18: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 647ms/step - accuracy: 0.9244 - loss: 0.5070 - val_accuracy: 0.9222 - val_loss: 0.5003\n",
      "Epoch 19/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - accuracy: 0.9453 - loss: 0.4723\n",
      "Epoch 19: val_accuracy improved from 0.94444 to 0.95556, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 672ms/step - accuracy: 0.9445 - loss: 0.4729 - val_accuracy: 0.9556 - val_loss: 0.4589\n",
      "Epoch 20/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520ms/step - accuracy: 0.9024 - loss: 0.5052\n",
      "Epoch 20: val_accuracy did not improve from 0.95556\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 635ms/step - accuracy: 0.9040 - loss: 0.5032 - val_accuracy: 0.9333 - val_loss: 0.4564\n",
      "Epoch 21/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525ms/step - accuracy: 0.9329 - loss: 0.4539\n",
      "Epoch 21: val_accuracy did not improve from 0.95556\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 640ms/step - accuracy: 0.9327 - loss: 0.4547 - val_accuracy: 0.9556 - val_loss: 0.4480\n",
      "Epoch 22/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - accuracy: 0.9376 - loss: 0.4459\n",
      "Epoch 22: val_accuracy did not improve from 0.95556\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 656ms/step - accuracy: 0.9375 - loss: 0.4455 - val_accuracy: 0.9444 - val_loss: 0.4353\n",
      "Epoch 23/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528ms/step - accuracy: 0.9201 - loss: 0.4482\n",
      "Epoch 23: val_accuracy did not improve from 0.95556\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 643ms/step - accuracy: 0.9198 - loss: 0.4482 - val_accuracy: 0.9111 - val_loss: 0.4395\n",
      "Epoch 24/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526ms/step - accuracy: 0.9066 - loss: 0.4389\n",
      "Epoch 24: val_accuracy improved from 0.95556 to 0.96667, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 660ms/step - accuracy: 0.9080 - loss: 0.4373 - val_accuracy: 0.9667 - val_loss: 0.3936\n",
      "Epoch 25/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - accuracy: 0.9362 - loss: 0.3988\n",
      "Epoch 25: val_accuracy did not improve from 0.96667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 655ms/step - accuracy: 0.9365 - loss: 0.3988 - val_accuracy: 0.9556 - val_loss: 0.4115\n",
      "Epoch 26/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530ms/step - accuracy: 0.9307 - loss: 0.3924\n",
      "Epoch 26: val_accuracy did not improve from 0.96667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 645ms/step - accuracy: 0.9306 - loss: 0.3925 - val_accuracy: 0.9444 - val_loss: 0.3984\n",
      "Epoch 27/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534ms/step - accuracy: 0.9628 - loss: 0.3602\n",
      "Epoch 27: val_accuracy did not improve from 0.96667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 648ms/step - accuracy: 0.9622 - loss: 0.3607 - val_accuracy: 0.9444 - val_loss: 0.3935\n",
      "Epoch 28/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - accuracy: 0.9452 - loss: 0.3519\n",
      "Epoch 28: val_accuracy did not improve from 0.96667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 656ms/step - accuracy: 0.9454 - loss: 0.3522 - val_accuracy: 0.9000 - val_loss: 0.3887\n",
      "Epoch 29/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528ms/step - accuracy: 0.9419 - loss: 0.3479\n",
      "Epoch 29: val_accuracy did not improve from 0.96667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 641ms/step - accuracy: 0.9421 - loss: 0.3488 - val_accuracy: 0.9333 - val_loss: 0.3891\n",
      "Epoch 30/30\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550ms/step - accuracy: 0.9602 - loss: 0.3352\n",
      "Epoch 30: val_accuracy did not improve from 0.96667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 678ms/step - accuracy: 0.9597 - loss: 0.3359 - val_accuracy: 0.9667 - val_loss: 0.3502\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Epoch 1/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582ms/step - accuracy: 0.8930 - loss: 0.4099\n",
      "Epoch 1: val_accuracy improved from -inf to 0.94444, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage2.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 857ms/step - accuracy: 0.8947 - loss: 0.4085 - val_accuracy: 0.9444 - val_loss: 0.3678 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555ms/step - accuracy: 0.9568 - loss: 0.3220\n",
      "Epoch 2: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 670ms/step - accuracy: 0.9565 - loss: 0.3225 - val_accuracy: 0.9333 - val_loss: 0.3559 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564ms/step - accuracy: 0.9334 - loss: 0.3203\n",
      "Epoch 3: val_accuracy improved from 0.94444 to 0.95556, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage2.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 697ms/step - accuracy: 0.9332 - loss: 0.3199 - val_accuracy: 0.9556 - val_loss: 0.3092 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.9312 - loss: 0.3019\n",
      "Epoch 4: val_accuracy did not improve from 0.95556\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 662ms/step - accuracy: 0.9316 - loss: 0.3014 - val_accuracy: 0.9444 - val_loss: 0.2875 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558ms/step - accuracy: 0.9636 - loss: 0.2550\n",
      "Epoch 5: val_accuracy improved from 0.95556 to 0.98889, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage2.weights.h5\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 690ms/step - accuracy: 0.9635 - loss: 0.2548 - val_accuracy: 0.9889 - val_loss: 0.2260 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565ms/step - accuracy: 0.9502 - loss: 0.2238\n",
      "Epoch 6: val_accuracy did not improve from 0.98889\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 682ms/step - accuracy: 0.9497 - loss: 0.2245 - val_accuracy: 0.9667 - val_loss: 0.2360 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555ms/step - accuracy: 0.9584 - loss: 0.2009\n",
      "Epoch 7: val_accuracy did not improve from 0.98889\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 670ms/step - accuracy: 0.9583 - loss: 0.2012 - val_accuracy: 0.9444 - val_loss: 0.2266 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545ms/step - accuracy: 0.9787 - loss: 0.1772\n",
      "Epoch 8: val_accuracy did not improve from 0.98889\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 662ms/step - accuracy: 0.9781 - loss: 0.1777 - val_accuracy: 0.9556 - val_loss: 0.2355 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546ms/step - accuracy: 0.9683 - loss: 0.1812\n",
      "Epoch 9: val_accuracy did not improve from 0.98889\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 661ms/step - accuracy: 0.9677 - loss: 0.1819 - val_accuracy: 0.9444 - val_loss: 0.2063 - learning_rate: 5.0000e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - accuracy: 0.9547 - loss: 0.2136\n",
      "Epoch 10: val_accuracy did not improve from 0.98889\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 659ms/step - accuracy: 0.9547 - loss: 0.2126 - val_accuracy: 0.9778 - val_loss: 0.1796 - learning_rate: 5.0000e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562ms/step - accuracy: 0.9642 - loss: 0.1816\n",
      "Epoch 11: val_accuracy did not improve from 0.98889\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 678ms/step - accuracy: 0.9642 - loss: 0.1816 - val_accuracy: 0.9333 - val_loss: 0.2157 - learning_rate: 5.0000e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559ms/step - accuracy: 0.9777 - loss: 0.1539\n",
      "Epoch 12: val_accuracy did not improve from 0.98889\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 674ms/step - accuracy: 0.9770 - loss: 0.1552 - val_accuracy: 0.9444 - val_loss: 0.2242 - learning_rate: 5.0000e-06\n",
      "Epoch 13/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553ms/step - accuracy: 0.9580 - loss: 0.1928\n",
      "Epoch 13: val_accuracy did not improve from 0.98889\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 668ms/step - accuracy: 0.9588 - loss: 0.1914 - val_accuracy: 0.9556 - val_loss: 0.1822 - learning_rate: 5.0000e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557ms/step - accuracy: 0.9522 - loss: 0.1764\n",
      "Epoch 14: val_accuracy did not improve from 0.98889\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 671ms/step - accuracy: 0.9522 - loss: 0.1765 - val_accuracy: 0.9778 - val_loss: 0.1631 - learning_rate: 2.5000e-06\n",
      "Epoch 15/20\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547ms/step - accuracy: 0.9516 - loss: 0.1706\n",
      "Epoch 15: val_accuracy did not improve from 0.98889\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 663ms/step - accuracy: 0.9523 - loss: 0.1702 - val_accuracy: 0.9778 - val_loss: 0.1563 - learning_rate: 2.5000e-06\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\myass\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 32 variables whereas the saved optimizer has 6 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Using best_stage2.weights.h5  —  Test Accuracy: 98.333%\n",
      "📂  Misclassified images copied to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\errors\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m cm       \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true, y_pred)\n\u001b[0;32m    146\u001b[0m cm_norm  \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m/\u001b[39m cm\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 148\u001b[0m \u001b[43mplot_confusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mCLASSES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF01_confusion.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m plot_confusion(cm_norm, CLASSES, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF02_confusion_norm.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# 标签分布\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 142\u001b[0m, in \u001b[0;36mplot_confusion\u001b[1;34m(cm, labels, norm, fname)\u001b[0m\n\u001b[0;32m    140\u001b[0m     txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(v)\n\u001b[0;32m    141\u001b[0m     ax\u001b[38;5;241m.\u001b[39mtext(j, i, txt, ha\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m, va\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 142\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout(); plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[43mOUT_DIR\u001b[49m\u001b[38;5;241m/\u001b[39mfname, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m); plt\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OUT_DIR' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "#  train_cnn_fruit_v3.py  –  EfficientNet-B0 + Hard-Aug\n",
    "#  • 取消 rescale，改用 preprocess_input\n",
    "#  • 默认关闭 MixUp (如需再开，把 USE_MIXUP=True 并参照上次补丁加入 simple_mixup)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import os, random, pathlib, shutil, numpy as np, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED']='42'; os.environ['TF_DETERMINISTIC_OPS']='1'\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED); tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "ROOT = r'C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5'\n",
    "TRAIN= pathlib.Path(ROOT,'train'); TEST = pathlib.Path(ROOT,'test')\n",
    "OUT  = pathlib.Path(ROOT,'result'); OUT.mkdir(parents=True, exist_ok=True)\n",
    "ERRDIR = OUT/'errors'; ERRDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (224,224); BATCH=32\n",
    "USE_MIXUP = False         \n",
    "EFF_PREP = tf.keras.applications.efficientnet.preprocess_input   \n",
    "\n",
    "def make_generators():\n",
    "    train_gen = ImageDataGenerator(\n",
    "        preprocessing_function=EFF_PREP,                  \n",
    "        validation_split=0.15,\n",
    "        rotation_range=30, width_shift_range=.2, height_shift_range=.2,\n",
    "        shear_range=.15, zoom_range=.2,\n",
    "        channel_shift_range=40., brightness_range=[.5,1.5],\n",
    "        horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    train_ds = train_gen.flow_from_directory(\n",
    "        TRAIN, target_size=IMG_SIZE, batch_size=BATCH,\n",
    "        subset='training', shuffle=True, seed=SEED)\n",
    "\n",
    "    val_ds   = train_gen.flow_from_directory(\n",
    "        TRAIN, target_size=IMG_SIZE, batch_size=BATCH,\n",
    "        subset='validation', shuffle=False, seed=SEED)\n",
    "\n",
    "    test_gen = ImageDataGenerator(preprocessing_function=EFF_PREP) ### ← MOD\n",
    "    test_ds  = test_gen.flow_from_directory(\n",
    "        TEST, target_size=IMG_SIZE, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds, val_ds, test_ds = make_generators()\n",
    "CLASSES = list(test_ds.class_indices.keys())\n",
    "\n",
    "base = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, input_shape=IMG_SIZE+(3,), weights='imagenet')\n",
    "base.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3, seed=SEED),\n",
    "    layers.Dense(len(CLASSES), activation='softmax')\n",
    "])\n",
    "model.compile(tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "ckpt1 = ModelCheckpoint(OUT/'best_stage1.weights.h5', save_weights_only=True,\n",
    "                        monitor='val_accuracy', mode='max',\n",
    "                        save_best_only=True, verbose=1)\n",
    "early1= EarlyStopping(monitor='val_accuracy', mode='max',\n",
    "                      patience=7, restore_best_weights=True, verbose=1)\n",
    "\n",
    "model.fit(train_ds, epochs=30, validation_data=val_ds,\n",
    "          callbacks=[ckpt1, early1])\n",
    "\n",
    "\n",
    "base.trainable=True; cnt=0\n",
    "for layer in reversed(base.layers):\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable=False\n",
    "    elif cnt<20:\n",
    "        layer.trainable=True; cnt+=1\n",
    "    else:\n",
    "        layer.trainable=False\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "ckpt2 = ModelCheckpoint(OUT/'best_stage2.weights.h5', save_weights_only=True,\n",
    "                        monitor='val_accuracy', mode='max',\n",
    "                        save_best_only=True, verbose=1)\n",
    "reduceL= ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                           patience=3, min_lr=2e-6, verbose=1)\n",
    "early2 = EarlyStopping(monitor='val_accuracy', mode='max',\n",
    "                       patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "model.fit(train_ds, epochs=20, validation_data=val_ds,\n",
    "          callbacks=[ckpt2, reduceL, early2])\n",
    "\n",
    "get_acc=lambda w: (model.load_weights(w), model.evaluate(test_ds,verbose=0)[1])[1]\n",
    "acc1=get_acc(OUT/'best_stage1.weights.h5')\n",
    "acc2=get_acc(OUT/'best_stage2.weights.h5')\n",
    "best = OUT/'best_stage2.weights.h5' if acc2>=acc1 else OUT/'best_stage1.weights.h5'\n",
    "model.load_weights(best)\n",
    "print(f'✅  Using {best.name}  —  Test Accuracy: {max(acc1,acc2):.3%}')\n",
    "\n",
    "y_true = test_ds.classes; y_prob=model.predict(test_ds,verbose=0)\n",
    "y_pred = y_prob.argmax(axis=1)\n",
    "for p in np.array(test_ds.filepaths)[y_true!=y_pred]:\n",
    "    shutil.copy(p, ERRDIR/pathlib.Path(p).name)\n",
    "print(f'📂  Misclassified images copied to {ERRDIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160a62e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\myass\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  CSV saved to  C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\metrics_summary.csv\n",
      "📦  All results zipped → C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\results_20250608_1939.zip\n",
      "   现在可直接在资源管理器中右键下载或复制。\n"
     ]
    }
   ],
   "source": [
    "# export_results.py  ——  运行前确保已 pip install pandas\n",
    "import numpy as np, pandas as pd, datetime, zipfile, pathlib, shutil, os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "ROOT   = r'C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5'\n",
    "OUT    = pathlib.Path(ROOT, 'result')\n",
    "BEST_W = OUT / 'best_stage2.weights.h5'   # ← 若脚本选了 stage1，请改名\n",
    "\n",
    "# ───────────────── 1  重新构建数据集 & 模型 ─────────────────\n",
    "IMG_SIZE = (224,224); BATCH=32\n",
    "PREP = tf.keras.applications.efficientnet.preprocess_input\n",
    "\n",
    "test_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "             preprocessing_function=PREP)\n",
    "test_ds  = test_gen.flow_from_directory(\n",
    "             pathlib.Path(ROOT,'test'), target_size=IMG_SIZE,\n",
    "             batch_size=BATCH, shuffle=False)\n",
    "\n",
    "CLASSES = list(test_ds.class_indices.keys())\n",
    "\n",
    "base = tf.keras.applications.EfficientNetB0(\n",
    "         include_top=False, input_shape=IMG_SIZE+(3,), weights=None)\n",
    "model = tf.keras.Sequential([\n",
    "    base, tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "])\n",
    "model.load_weights(BEST_W)\n",
    "\n",
    "# ───────────────── 2  预测 & 指标表 ─────────────────\n",
    "y_true = test_ds.classes\n",
    "y_prob = model.predict(test_ds, verbose=0)\n",
    "y_pred = y_prob.argmax(axis=1)\n",
    "\n",
    "cm   = confusion_matrix(y_true, y_pred)\n",
    "rep  = classification_report(\n",
    "         y_true, y_pred, target_names=CLASSES, output_dict=True)\n",
    "\n",
    "# DataFrame（每类 + macro + weighted + accuracy）\n",
    "rows  = CLASSES + ['macro avg','weighted avg']\n",
    "data  = {k:[rep[k]['precision'], rep[k]['recall'], rep[k]['f1-score']]\n",
    "         for k in rows if k in rep}\n",
    "df    = pd.DataFrame(data, index=['precision','recall','f1']).T\n",
    "df['support'] = [rep[k]['support'] for k in rows]\n",
    "\n",
    "acc = np.mean(y_true == y_pred)\n",
    "df.loc['accuracy (%)','f1'] = acc * 100\n",
    "\n",
    "csv_path = OUT/'metrics_summary.csv'\n",
    "df.to_csv(csv_path, float_format='%.4f')\n",
    "np.save(OUT/'confusion_matrix.npy', cm)\n",
    "\n",
    "print(f'✅  CSV saved to  {csv_path}')\n",
    "\n",
    "# ───────────────── 3  打包 ZIP ─────────────────\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "zip_path  = OUT/f'results_{timestamp}.zip'\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "    # a) 图表 PNG\n",
    "    for png in OUT.glob('F0*.png'):\n",
    "        z.write(png, arcname=png.name)\n",
    "    # b) 指标 CSV + NPY\n",
    "    z.write(csv_path, arcname=csv_path.name)\n",
    "    z.write(OUT/'confusion_matrix.npy', arcname='confusion_matrix.npy')\n",
    "    # c) 错误图片文件夹（如果有）\n",
    "    err_dir = OUT/'errors'\n",
    "    if err_dir.exists():\n",
    "        for p in err_dir.glob('*'):\n",
    "            z.write(p, arcname=f'errors/{p.name}')\n",
    "\n",
    "print(f'📦  All results zipped → {zip_path}')\n",
    "print('   现在可直接在资源管理器中右键下载或复制。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4d1379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520 images belonging to 4 classes.\n",
      "Found 90 images belonging to 4 classes.\n",
      "Found 60 images belonging to 4 classes.\n",
      "Epoch 1/30\n",
      " 5/17 [=======>......................] - ETA: 9s - loss: 1.5994 - accuracy: 0.1500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\myass\\lib\\site-packages\\PIL\\Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 1.5810 - accuracy: 0.1500\n",
      "Epoch 1: val_accuracy improved from -inf to 0.13333, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 19s 982ms/step - loss: 1.5810 - accuracy: 0.1500 - val_loss: 1.4332 - val_accuracy: 0.1333\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4442 - accuracy: 0.2481\n",
      "Epoch 2: val_accuracy improved from 0.13333 to 0.26667, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 897ms/step - loss: 1.4442 - accuracy: 0.2481 - val_loss: 1.3453 - val_accuracy: 0.2667\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3670 - accuracy: 0.2808\n",
      "Epoch 3: val_accuracy improved from 0.26667 to 0.43333, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 884ms/step - loss: 1.3670 - accuracy: 0.2808 - val_loss: 1.2470 - val_accuracy: 0.4333\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2820 - accuracy: 0.3962\n",
      "Epoch 4: val_accuracy improved from 0.43333 to 0.54444, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 16s 936ms/step - loss: 1.2820 - accuracy: 0.3962 - val_loss: 1.1654 - val_accuracy: 0.5444\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1750 - accuracy: 0.5000\n",
      "Epoch 5: val_accuracy improved from 0.54444 to 0.64444, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 910ms/step - loss: 1.1750 - accuracy: 0.5000 - val_loss: 1.1007 - val_accuracy: 0.6444\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0971 - accuracy: 0.5788\n",
      "Epoch 6: val_accuracy improved from 0.64444 to 0.73333, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 906ms/step - loss: 1.0971 - accuracy: 0.5788 - val_loss: 1.0447 - val_accuracy: 0.7333\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0432 - accuracy: 0.6385\n",
      "Epoch 7: val_accuracy improved from 0.73333 to 0.78889, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 879ms/step - loss: 1.0432 - accuracy: 0.6385 - val_loss: 0.9531 - val_accuracy: 0.7889\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0073 - accuracy: 0.6654\n",
      "Epoch 8: val_accuracy improved from 0.78889 to 0.81111, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 903ms/step - loss: 1.0073 - accuracy: 0.6654 - val_loss: 0.9154 - val_accuracy: 0.8111\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9523 - accuracy: 0.7077\n",
      "Epoch 9: val_accuracy improved from 0.81111 to 0.83333, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 887ms/step - loss: 0.9523 - accuracy: 0.7077 - val_loss: 0.8703 - val_accuracy: 0.8333\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8871 - accuracy: 0.7808\n",
      "Epoch 10: val_accuracy improved from 0.83333 to 0.86667, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 900ms/step - loss: 0.8871 - accuracy: 0.7808 - val_loss: 0.8217 - val_accuracy: 0.8667\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8301 - accuracy: 0.8000\n",
      "Epoch 11: val_accuracy improved from 0.86667 to 0.90000, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 904ms/step - loss: 0.8301 - accuracy: 0.8000 - val_loss: 0.7721 - val_accuracy: 0.9000\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7738 - accuracy: 0.8365\n",
      "Epoch 12: val_accuracy improved from 0.90000 to 0.95556, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage1.weights.h5\n",
      "17/17 [==============================] - 15s 891ms/step - loss: 0.7738 - accuracy: 0.8365 - val_loss: 0.7309 - val_accuracy: 0.9556\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7415 - accuracy: 0.8385\n",
      "Epoch 13: val_accuracy did not improve from 0.95556\n",
      "17/17 [==============================] - 15s 884ms/step - loss: 0.7415 - accuracy: 0.8385 - val_loss: 0.7212 - val_accuracy: 0.8889\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7241 - accuracy: 0.8500\n",
      "Epoch 14: val_accuracy did not improve from 0.95556\n",
      "17/17 [==============================] - 15s 891ms/step - loss: 0.7241 - accuracy: 0.8500 - val_loss: 0.6530 - val_accuracy: 0.9333\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.8481\n",
      "Epoch 15: val_accuracy did not improve from 0.95556\n",
      "17/17 [==============================] - 15s 890ms/step - loss: 0.6874 - accuracy: 0.8481 - val_loss: 0.6622 - val_accuracy: 0.9444\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6641 - accuracy: 0.8769\n",
      "Epoch 16: val_accuracy did not improve from 0.95556\n",
      "17/17 [==============================] - 15s 910ms/step - loss: 0.6641 - accuracy: 0.8769 - val_loss: 0.6126 - val_accuracy: 0.9000\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6346 - accuracy: 0.8904\n",
      "Epoch 17: val_accuracy did not improve from 0.95556\n",
      "17/17 [==============================] - 15s 909ms/step - loss: 0.6346 - accuracy: 0.8904 - val_loss: 0.6062 - val_accuracy: 0.8889\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6181 - accuracy: 0.8865\n",
      "Epoch 18: val_accuracy did not improve from 0.95556\n",
      "17/17 [==============================] - 16s 959ms/step - loss: 0.6181 - accuracy: 0.8865 - val_loss: 0.5737 - val_accuracy: 0.9222\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5855 - accuracy: 0.8962\n",
      "Epoch 19: val_accuracy did not improve from 0.95556\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "17/17 [==============================] - 15s 901ms/step - loss: 0.5855 - accuracy: 0.8962 - val_loss: 0.5553 - val_accuracy: 0.9444\n",
      "Epoch 19: early stopping\n",
      "Epoch 1/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7590 - accuracy: 0.8212\n",
      "Epoch 1: val_accuracy improved from -inf to 0.93333, saving model to C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5\\result\\best_stage2.weights.h5\n",
      "17/17 [==============================] - 21s 1s/step - loss: 0.7590 - accuracy: 0.8212 - val_loss: 0.6672 - val_accuracy: 0.9333 - lr: 1.0000e-05\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.8712\n",
      "Epoch 2: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 931ms/step - loss: 0.6559 - accuracy: 0.8712 - val_loss: 0.6146 - val_accuracy: 0.9111 - lr: 1.0000e-05\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6185 - accuracy: 0.8596\n",
      "Epoch 3: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 935ms/step - loss: 0.6185 - accuracy: 0.8596 - val_loss: 0.5520 - val_accuracy: 0.8889 - lr: 1.0000e-05\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.9019\n",
      "Epoch 4: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 925ms/step - loss: 0.5455 - accuracy: 0.9019 - val_loss: 0.4918 - val_accuracy: 0.9000 - lr: 1.0000e-05\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.8981\n",
      "Epoch 5: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 934ms/step - loss: 0.5028 - accuracy: 0.8981 - val_loss: 0.4421 - val_accuracy: 0.9333 - lr: 1.0000e-05\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.9135\n",
      "Epoch 6: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 927ms/step - loss: 0.4494 - accuracy: 0.9135 - val_loss: 0.4133 - val_accuracy: 0.9333 - lr: 1.0000e-05\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.9096\n",
      "Epoch 7: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 937ms/step - loss: 0.4204 - accuracy: 0.9096 - val_loss: 0.3889 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3728 - accuracy: 0.9250\n",
      "Epoch 8: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 928ms/step - loss: 0.3728 - accuracy: 0.9250 - val_loss: 0.3446 - val_accuracy: 0.9333 - lr: 1.0000e-05\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3558 - accuracy: 0.9385\n",
      "Epoch 9: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 930ms/step - loss: 0.3558 - accuracy: 0.9385 - val_loss: 0.3330 - val_accuracy: 0.8889 - lr: 1.0000e-05\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.9308\n",
      "Epoch 10: val_accuracy did not improve from 0.93333\n",
      "17/17 [==============================] - 16s 939ms/step - loss: 0.3222 - accuracy: 0.9308 - val_loss: 0.3238 - val_accuracy: 0.9111 - lr: 1.0000e-05\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.9212\n",
      "Epoch 11: val_accuracy did not improve from 0.93333\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 16s 933ms/step - loss: 0.3151 - accuracy: 0.9212 - val_loss: 0.2923 - val_accuracy: 0.9333 - lr: 1.0000e-05\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 101\u001b[0m\n\u001b[0;32m     97\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_ds, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39mval_ds,\n\u001b[0;32m     98\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39m[ckpt2, reduceL, early2])\n\u001b[0;32m    100\u001b[0m get_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m w: (model\u001b[38;5;241m.\u001b[39mload_weights(w), model\u001b[38;5;241m.\u001b[39mevaluate(test_ds,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 101\u001b[0m acc1\u001b[38;5;241m=\u001b[39m\u001b[43mget_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUT\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_stage1.weights.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m acc2\u001b[38;5;241m=\u001b[39mget_acc(OUT\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_stage2.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    103\u001b[0m best \u001b[38;5;241m=\u001b[39m OUT\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_stage2.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m acc2\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39macc1 \u001b[38;5;28;01melse\u001b[39;00m OUT\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_stage1.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 100\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m     94\u001b[0m early2 \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     95\u001b[0m                        patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     97\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_ds, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39mval_ds,\n\u001b[0;32m     98\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39m[ckpt2, reduceL, early2])\n\u001b[1;32m--> 100\u001b[0m get_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m w: (\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m, model\u001b[38;5;241m.\u001b[39mevaluate(test_ds,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    101\u001b[0m acc1\u001b[38;5;241m=\u001b[39mget_acc(OUT\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_stage1.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    102\u001b[0m acc2\u001b[38;5;241m=\u001b[39mget_acc(OUT\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_stage2.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\myass\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\myass\\lib\\site-packages\\numpy\\core\\fromnumeric.py:655\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(a, axes)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtranspose\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m \n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranspose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\myass\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "#  train_cnn_fruit_v3.py  –  EfficientNet-B0 + Hard-Aug\n",
    "#  • 取消 rescale，改用 preprocess_input\n",
    "#  • 默认关闭 MixUp (如需再开，把 USE_MIXUP=True 并参照上次补丁加入 simple_mixup)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import os, random, pathlib, shutil, numpy as np, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED']='42'; os.environ['TF_DETERMINISTIC_OPS']='1'\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED); tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "ROOT = r'C:\\Users\\Cleveland\\Desktop\\mechanical\\assignment5'\n",
    "TRAIN= pathlib.Path(ROOT,'train'); TEST = pathlib.Path(ROOT,'test')\n",
    "OUT  = pathlib.Path(ROOT,'result'); OUT.mkdir(parents=True, exist_ok=True)\n",
    "ERRDIR = OUT/'errors'; ERRDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (224,224); BATCH=32\n",
    "USE_MIXUP = False         \n",
    "EFF_PREP = tf.keras.applications.efficientnet.preprocess_input   \n",
    "\n",
    "def make_generators():\n",
    "    train_gen = ImageDataGenerator(\n",
    "        preprocessing_function=EFF_PREP,                  \n",
    "        validation_split=0.15,\n",
    "        rotation_range=30, width_shift_range=.2, height_shift_range=.2,\n",
    "        shear_range=.15, zoom_range=.2,\n",
    "        channel_shift_range=40., brightness_range=[.5,1.5],\n",
    "        horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    train_ds = train_gen.flow_from_directory(\n",
    "        TRAIN, target_size=IMG_SIZE, batch_size=BATCH,\n",
    "        subset='training', shuffle=True, seed=SEED)\n",
    "\n",
    "    val_ds   = train_gen.flow_from_directory(\n",
    "        TRAIN, target_size=IMG_SIZE, batch_size=BATCH,\n",
    "        subset='validation', shuffle=False, seed=SEED)\n",
    "\n",
    "    test_gen = ImageDataGenerator(preprocessing_function=EFF_PREP) ### ← MOD\n",
    "    test_ds  = test_gen.flow_from_directory(\n",
    "        TEST, target_size=IMG_SIZE, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds, val_ds, test_ds = make_generators()\n",
    "CLASSES = list(test_ds.class_indices.keys())\n",
    "\n",
    "base = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, input_shape=IMG_SIZE+(3,), weights='imagenet')\n",
    "base.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3, seed=SEED),\n",
    "    layers.Dense(len(CLASSES), activation='softmax')\n",
    "])\n",
    "model.compile(tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "ckpt1 = ModelCheckpoint(OUT/'best_stage1.weights.h5', save_weights_only=True,\n",
    "                        monitor='val_accuracy', mode='max',\n",
    "                        save_best_only=True, verbose=1)\n",
    "early1= EarlyStopping(monitor='val_accuracy', mode='max',\n",
    "                      patience=7, restore_best_weights=True, verbose=1)\n",
    "\n",
    "model.fit(train_ds, epochs=30, validation_data=val_ds,\n",
    "          callbacks=[ckpt1, early1])\n",
    "\n",
    "\n",
    "base.trainable=True; cnt=0\n",
    "for layer in reversed(base.layers):\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable=False\n",
    "    elif cnt<20:\n",
    "        layer.trainable=True; cnt+=1\n",
    "    else:\n",
    "        layer.trainable=False\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "ckpt2 = ModelCheckpoint(OUT/'best_stage2.weights.h5', save_weights_only=True,\n",
    "                        monitor='val_accuracy', mode='max',\n",
    "                        save_best_only=True, verbose=1)\n",
    "reduceL= ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                           patience=3, min_lr=2e-6, verbose=1)\n",
    "early2 = EarlyStopping(monitor='val_accuracy', mode='max',\n",
    "                       patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "model.fit(train_ds, epochs=20, validation_data=val_ds,\n",
    "          callbacks=[ckpt2, reduceL, early2])\n",
    "\n",
    "get_acc=lambda w: (model.load_weights(w), model.evaluate(test_ds,verbose=0)[1])[1]\n",
    "acc1=get_acc(OUT/'best_stage1.weights.h5')\n",
    "acc2=get_acc(OUT/'best_stage2.weights.h5')\n",
    "best = OUT/'best_stage2.weights.h5' if acc2>=acc1 else OUT/'best_stage1.weights.h5'\n",
    "model.load_weights(best)\n",
    "print(f'✅  Using {best.name}  —  Test Accuracy: {max(acc1,acc2):.3%}')\n",
    "\n",
    "y_true = test_ds.classes; y_prob=model.predict(test_ds,verbose=0)\n",
    "y_pred = y_prob.argmax(axis=1)\n",
    "for p in np.array(test_ds.filepaths)[y_true!=y_pred]:\n",
    "    shutil.copy(p, ERRDIR/pathlib.Path(p).name)\n",
    "print(f'📂  Misclassified images copied to {ERRDIR}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
